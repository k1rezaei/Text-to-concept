{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a1c0079",
   "metadata": {},
   "source": [
    "## CIFAR-10 zero-shot classifier with Resnet50\n",
    "In this notebook, we provide a use-case of our method.\n",
    "We use a Resnet50 model (pre-trained on ImageNet) and leverage Text-To-Concept to turn it into a zero-shot classifer on CIFAR-10.\n",
    "This notebook has these sections:\n",
    "+ <i>Preliminaries</i>: we import required libraries and load transformations.\n",
    "+ <i>Resnet50</i>: we load the model and implement its necessary functions, enabling us to use `TextToConcept` framework.\n",
    "+ <i>Linear Aligner</i>: we initiate `TextToConcept` object and train/load its linear aligner.\n",
    "+ <i>Zero-shot classifier</i>: we use methods implemented in `TextToConcept` and appropriate text prompts to get the zero-shot classifer.\n",
    "+ <i>Zero-shot performance on CIFAR-10</i>: we load CIFAR-10 and evaluate Resnet50-based zero-shot classifier on it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7cce84",
   "metadata": {},
   "source": [
    "### Preliminaries\n",
    "In this section, we import the required libraries and initialize standard transformations necessary for loading datasets. It is worth mentioning that certain models require input normalization, while others do not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f177ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from TextToConcept import TextToConcept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d06c9913",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "306b3426",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5dc3451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_transform_without_normalization = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(224),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor()])\n",
    "\n",
    "\n",
    "std_transform_with_normalization = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(224),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(), \n",
    "    torchvision.transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa435299",
   "metadata": {},
   "source": [
    "### Resnet50\n",
    "In this part, we load Resnet50 model.\n",
    "In order to use ``TextToConcept`` framework, model should implement these functions/attributes:\n",
    "+ ``forward_features(x)`` that takes a tensor as the input and outputs the representation (features) of input $x$ when it is passed through the model.\n",
    "+ ``get_normalizer`` should be the normalizer that the models uses to preprocess the input. e.g., Resnet18, uses standard ImageNet normalizer.\n",
    "+ Attribute ``has_normalizer`` should be `True` when normalizer is need for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd2b54d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "encoder = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "model.forward_features = lambda x : encoder(x)\n",
    "model.get_normalizer = torchvision.transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "model.has_normalizer = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da8df15",
   "metadata": {},
   "source": [
    "### Linear Aligner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4b4fe7",
   "metadata": {},
   "source": [
    "<b>Initiating Text-To-Concept Object</b><br>\n",
    "In this section, we initiate ``TextToConcept`` object which turns the vision encoder (e.g., Resnet50) into a model capable of integrating language and vision. By doing so, we enable the utilization of certain abilities present in vision-language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "527a1b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_concept = TextToConcept(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01c48fb",
   "metadata": {},
   "source": [
    "We can either train the aligner or load an existing one.\n",
    "\n",
    "#### Training Linear Aligner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce244e3",
   "metadata": {},
   "source": [
    "<b>Loading ImageNet Dataset to Train the Aligner</b><br>\n",
    "We note that even $20\\%$ of ImageNet training samples suffices for training an effective linear aligner. \n",
    "We refer to Appendix A of our paper for more details on sample efficiency of linear alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1e82a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading imagenet dataset to train aligner.\n",
    "dset = torchvision.datasets.ImageNet(root='/fs/cml-datasets/ImageNet/ILSVRC2012',\n",
    "                                     split='train',\n",
    "                                     transform=std_transform_without_normalization)\n",
    "\n",
    "# 20% of images are fairly enough.\n",
    "num_of_samples = int(0.02 * len(dset))\n",
    "dset = torch.utils.data.Subset(dset, np.random.choice(np.arange(len(dset)), num_of_samples, replace=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65e6389",
   "metadata": {},
   "source": [
    "<b>Training the Linear Aligner</b><br>\n",
    "After loading the object, we need to train the aligner.\n",
    "+ In order to train the aligner, ``train_linear_aligner`` should be called which obtains representations of the given model (e.g., Resnet50) on ``dset`` as well that of a vision-language model such as CLIP. These representations can also be loaded. Next, this function solves the linear transformation and obtain optimal alignment from model's space to vision-language space.\n",
    "+ By calling the function ``save_linear_aligner``, linear aliger will be stored which can be utilized later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e492de01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading representations ...\n",
      "Training linear aligner ...\n",
      "Linear alignment: ((500000, 2048)) --> ((500000, 512)).\n",
      "Initial MSE, R^2: 7.130, -0.584\n",
      "Epoch number, loss: 0, 1.304\n",
      "Epoch number, loss: 1, 1.196\n",
      "Epoch number, loss: 2, 1.180\n",
      "Epoch number, loss: 3, 1.175\n",
      "Epoch number, loss: 4, 1.173\n",
      "Final MSE, R^2 = 1.167, 0.741\n"
     ]
    }
   ],
   "source": [
    "path_to_model = '/cmlscratch/mmoayeri/ftr_alignment/saved_ftrs/pytorch_resnet50_500samples_per_class.npy'\n",
    "path_to_clip_model = '/cmlscratch/mmoayeri/ftr_alignment/saved_ftrs/clip_ViT-B_16_500samples_per_class.npy'\n",
    "\n",
    "text_to_concept.train_linear_aligner(dset,\n",
    "                                     load_reps=True,\n",
    "                                     path_to_model=path_to_model,\n",
    "                                     path_to_clip_model=path_to_clip_model)\n",
    "\n",
    "text_to_concept.save_linear_aligner('imagenet_resnet50_aligner.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02221daa",
   "metadata": {},
   "source": [
    "<b>Loading the Linear Aligner</b><br>\n",
    "We can also use an already existing linear aligner, to do so, we use the function ``load_linear_aligner``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e3b38e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_concept.load_linear_aligner('imagenet_resnet50_aligner.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aae71d",
   "metadata": {},
   "source": [
    "### Zero-shot Classifier\n",
    "We note that CIFAR-10 is a <i>$10$-way</i> classification problem. \n",
    "We use prompts of the form `a pixelated of {c}` to get appropriate concepts in vision-language space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3acb2674",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c01e7538",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_zeroshot_classifier = text_to_concept.get_zero_shot_classifier(cifar_classes,\n",
    "                                                                     prompts=['a pixelated photo of a {}'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b02859f",
   "metadata": {},
   "source": [
    "### Zero-shot performance on CIFAR-10\n",
    "After loading CIFAR-10, we use `cifar_zeroshot_classifier(x)` to get logits of the classification problem when input $x$ is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6492d680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar = torchvision.datasets.CIFAR10(root='data/',\n",
    "                                     download=True,\n",
    "                                     train=False,\n",
    "                                     transform=std_transform_with_normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "571e6044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 625/625 [00:19<00:00, 32.83it/s]\n"
     ]
    }
   ],
   "source": [
    "loader = torch.utils.data.DataLoader(cifar, batch_size=16, shuffle=True, num_workers=8)\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(loader):\n",
    "        x, y = data[:2]\n",
    "        x = x.to(device)\n",
    "\n",
    "        outputs = cifar_zeroshot_classifier(x).detach().cpu()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += y.size(0)\n",
    "        correct += predicted.eq(y).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4c086a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ResNet50 Zeroshot Accuracy on CIFAR-10 67.54'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'ResNet50 Zeroshot Accuracy on CIFAR-10 {100.*correct/total:.2f}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
